{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of custom gym.Env\n",
    "from gym_env.envs.perturbation_world import PerturbationEnv\n",
    "\n",
    "# Import checker to ensure environment is suitable for StableBaselines usage\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Checks our policy and returns information about it\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import reinforcement learning algorithm library\n",
    "from stable_baselines3 import A2C, DDPG, HER, SAC, TD3, PPO\n",
    "\n",
    "# Needed for creating new directories\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create playground\n",
    "from resources.create_playground import createPlayground\n",
    "\n",
    "# Simplest playground, no perturbation\n",
    "playground1 = createPlayground(\n",
    "    (True, True),\n",
    "    [\n",
    "        [(-100, 30), (True, True)],\n",
    "        [(100, 10), (False, True)],\n",
    "        [(100, 100), (False, False)],\n",
    "        [(-100, -100), (True, False)],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load and initialize environment\n",
    "env = PerturbationEnv(playground1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Testing the environment\n",
    "# Code taken from: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\n",
    "# If no errors occur, our environment is suitable for usage\n",
    "env.reset()\n",
    "check_env(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examples of action and observation\n",
    "env.reset()\n",
    "print(\"sample action:\", env.action_space.sample())\n",
    "print(\"sample space shape:\", env.action_space.shape)\n",
    "print(\"observation space shape:\", env.observation_space.shape)\n",
    "print(\"sample observation:\", env.observation_space.sample())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example of agent randomly interacting in environment until it is considered as DONE (most likely running until the time limit has been reached)\n",
    "# env.render() plots the playground as an image per step\n",
    "# We can concatenate the image per step and store into one\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Prevents the environment from going past our predefined time_limit\n",
    "    if env.playground.timestep >= env.playground.time_limit:\n",
    "        done = True\n",
    "        break\n",
    "    else:\n",
    "        obs, reward, done, msg = env.step(env.action_space.sample())\n",
    "        print(\n",
    "            f\"TimeStep: {env.playground.timestep}, Observation: {obs}, Reward: {reward}, Done: {done}, Message: {msg}\"\n",
    "        )\n",
    "        env.render()\n",
    "print(f\"Completed in {env.playground.timestep} timesteps!\")\n",
    "env.save_images(\"random\")\n",
    "\n",
    "# A png containing each step of the environment will be available in IN3007/gym_env/results/pngs/random.png\n",
    "# Each step image will correspond to the information written from the output containing the timestep, observation of the agent, current reward the agent has obtained, if the environment is considered done, and any messages sent from the agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training a model, and saving the model\n",
    "\n",
    "# Create folders for storing models\n",
    "models_dir = Path(\"models/PPO\")\n",
    "logdir = \"logs\"\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# We split the training per 1000 timesteps so that we can choose the best point of time to load the model from\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "for i in range(1, 30):\n",
    "    model.learn(total_timesteps=env.playground.time_limit, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(Path(f\"{models_dir}/{env.playground.time_limit*i}\"))\n",
    "del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = Path(f\"{models_dir}/9000.zip\")\n",
    "model = PPO.load(path=model_path, env=env, print_system_info=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean Reward: {mean_reward}, std_reward: {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "results = []\n",
    "for episode in range(1, 10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if env.playground.timestep >= env.playground.time_limit:\n",
    "            done = True\n",
    "            break\n",
    "        else:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "        print(f\"Test {episode}, current timestep: {env.playground.timestep} Reward: {reward}, Done: {done}\")\n",
    "    results.append([env.playground.timestep, reward, done])\n",
    "    env.save_images(f\"PPO_V0_{episode}\")\n",
    "print(f\"[Timestep, Reward, Done]\")\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A2C\n",
    "\n",
    "# Paths for models and logs folder, for convenience\n",
    "models_dir = \"models/A2C\"\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIME_STEPS = 10000\n",
    "for i in range(1, 10):\n",
    "    model.learn(\n",
    "        total_timesteps=TIME_STEPS, reset_num_timesteps=False, tb_log_name=\"A2C\"\n",
    "    )\n",
    "    model.save(f\"{models_dir}/{TIME_STEPS * i}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}