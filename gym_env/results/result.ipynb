{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of custom gym.Env\n",
    "from gym_env.envs.pertubation_world import PerturbationEnv\n",
    "\n",
    "# Import checker to ensure environment is suitable for StableBaselines usage\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Checks our policy and returns information about it\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import reinforcement learning algorithm library\n",
    "from stable_baselines3 import A2C, DDPG, HER, SAC, TD3, PPO\n",
    "\n",
    "# Create playground\n",
    "from resources.create_playground import createPlayground\n",
    "\n",
    "playground2 = createPlayground(\n",
    "    (True, True),\n",
    "    [\n",
    "        [(-100, 30), (True, True)],\n",
    "        [(100, 10), (False, True)],\n",
    "        [(100, 100), (False, False)],\n",
    "        [(-100, -100), (True, False)],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load and initialize environment\n",
    "env = PerturbationEnv(playground2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Testing the environment\n",
    "# Code taken from: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\n",
    "# If no errors occur, our environment is suitable for usage\n",
    "env.reset()\n",
    "check_env(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Examples of action and observation\n",
    "env.reset()\n",
    "print(\"sample action:\", env.action_space.sample())\n",
    "print(\"sample space shape:\", env.action_space.shape)\n",
    "print(\"observation space shape:\", env.observation_space.shape)\n",
    "print(\"sample observation:\", env.observation_space.sample())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example of agent randomly interacting in environment until it is considered as DONE (or more likely, until the time limit has been reached)\n",
    "# env.render() plots the playground as an image per step\n",
    "# We can concatenate the image per step and store into one\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, msg = env.step(env.action_space.sample())\n",
    "    print(\n",
    "        f\"TimeStep: {env.playground.timestep}, Observation: {obs}, Reward: {reward}, Done: {done}, Message: {msg}\"\n",
    "    )\n",
    "    env.render()\n",
    "print(f\"Completed in {env.playground.timestep} timesteps!\")\n",
    "env.save_images(\"random\")\n",
    "\n",
    "# A png containing each step of the environment will be available in IN3007/gym_env/results/pngs/random.png\n",
    "# Each step image will correspond to the information written from the output containing the timestep, observation of the agent, current reward the agent has obtained, if the environment is considered done, and any messages sent from the agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training a model, and saving the model\n",
    "\n",
    "# Paths for models and logs folder, for convenience\n",
    "models_dir = \"models/PPO\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "for i in range(1, 30):\n",
    "    model.learn(\n",
    "        total_timesteps=env.playground.time_limit,\n",
    "        reset_num_timesteps=False,\n",
    "        tb_log_name=\"PPO\",\n",
    "    )\n",
    "    model.save(f\"{models_dir}/{env.playground.time_limit*i}\")\n",
    "del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = f\"{models_dir}/9000.zip\"\n",
    "model = PPO.load(path=model_path, env=env, print_system_info=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean Reward: {mean_reward}, std_reward: {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "results = []\n",
    "for i in range(1, 10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    results.append(reward)\n",
    "    env.save_images(f\"PPO_V0_{i}\")\n",
    "print(results)\n",
    "\n",
    "# # Enjoy trained agent\n",
    "# episodes = 10\n",
    "#\n",
    "# for ep in range(episodes):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action, _states = model.predict(obs)\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         env.save_images(f\"PPO_V0_ep{episodes}\")\n",
    "\n",
    "# obs = vec_env.reset()\n",
    "# episodes = 1000\n",
    "# for i in range(episodes):\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "#     vec_env.render()\n",
    "#     #env.save_images(f\"trainV0{episodes}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A2C\n",
    "\n",
    "# Paths for models and logs folder, for convenience\n",
    "models_dir = \"models/A2C\"\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log=logdir)\n",
    "\n",
    "TIME_STEPS = 10000\n",
    "for i in range(1, 10):\n",
    "    model.learn(\n",
    "        total_timesteps=TIME_STEPS, reset_num_timesteps=False, tb_log_name=\"A2C\"\n",
    "    )\n",
    "    model.save(f\"{models_dir}/{TIME_STEPS * i}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}